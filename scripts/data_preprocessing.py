# -*- coding: utf-8 -*-
"""DATA_PREPROCESSING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XhtNKbzSq59Kve7O-dFn3YS5f2x1AmAt
"""

import json
import logging
import os
from typing import Dict, List, Any, Optional, Union
import re
import pandas as pd
import numpy as np

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_json_data(file_path):
  try:
    with open(file_path,'r',encoding='utf-8') as f:
      data = json.load(f)
      logger.info(f"Successfully loaded data from {file_path}")
      return data
  except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
  except json.JSONDecodeError:
        logger.error(f"Invalid JSON in file: {file_path}")
        raise

def extract_video_features(video_data) :
  videos=[]

  for item in video_data.get('items',[]):
        video={}
        video['video_id']=item.get('id','')
        video['title']=item.get('snippet',{}).get('title','')
        video['description']=item.get('snippet',{}).get('description','')
        video['channel_title']=item.get('snippet',{}).get('channelTitle','')
        video['published_at'] = item.get('snippet',{}).get('publishedAt', '')
        video['channel_id'] = item.get('snippet',{}).get('channelId', '')

        content_details = item.get('contentDetails', {})
        video['duration'] = content_details.get('duration', '')
        video['dimension'] = content_details.get('dimension', '')
        video['definition'] = content_details.get('definition', '')

        statistics = item.get('statistics', {})
        video['view_count'] = int(statistics.get('viewCount', 0))
        video['like_count'] = int(statistics.get('likeCount', 0))
        video['comment_count'] = int(statistics.get('commentCount', 0))
        video['favorite_count'] = int(statistics.get('favoriteCount', 0))
        video['dislike_count'] = int(statistics.get('dislikeCount', 0))

        videos.append(video)

  logger.info(f"Extracted features for {len(videos)} videos")
  return videos

def convert_duration_to_seconds(duration) :
  if not duration:
    return 0
  duration=duration.replace('PT','')
  hours = re.search(r'(\d+)H', duration)
  minutes = re.search(r'(\d+)M', duration)
  seconds = re.search(r'(\d+)S', duration)
  total_seconds = 0
  if hours:
        total_seconds += int(hours.group(1)) * 3600
  if minutes:
        total_seconds += int(minutes.group(1)) * 60
  if seconds:
        total_seconds += int(seconds.group(1))

  return total_seconds

def create_videos_dataframe(videos):
  df=pd.DataFrame(videos)
  if 'published_at' in df.columns:
    df['published_at']=pd.to_datetime(df['published_at'])
  if 'duration' in df.columns:
    df['duration']=df['duration'].apply(convert_duration_to_seconds)
  if all(col in df.columns for col in ['view_count', 'like_count', 'comment_count']):
        df['likes_per_view'] = df['like_count'] / df['view_count'].replace(0, np.nan)
        df['comments_per_view'] = df['comment_count'] / df['view_count'].replace(0, np.nan)
        df['engagement_score'] = (df['like_count'] + df['comment_count'] * 2) / df['view_count'].replace(0, np.nan)

        df[['likes_per_view', 'comments_per_view', 'engagement_score']] = df[
            ['likes_per_view', 'comments_per_view', 'engagement_score']
        ].fillna(0)

  logger.info(f"Created DataFrame with {len(df)} rows and {len(df.columns)} columns")
  return df

def extract_comment_features(comment_data, video_id):
  comments=[]
  for item in comment_data.get('items',[]):
        comment={}
        comment['comment_id'] = item.get('id', '')
        comment['video_id'] = video_id

        snippet = item.get('snippet', {})
        top_level_comment = snippet.get('topLevelComment', {}).get('snippet', {})

        comment['text'] = top_level_comment.get('textDisplay', '')
        comment['author_name'] = top_level_comment.get('authorDisplayName', '')
        comment['author_channel_id'] = top_level_comment.get('authorChannelId', {}).get('value', '')
        comment['published_at'] = top_level_comment.get('publishedAt', '')

        comment['like_count'] = int(top_level_comment.get('likeCount', 0))
        comment['reply_count'] = int(snippet.get('totalReplyCount', 0))

        comments.append(comment)

  logger.info(f"Extracted features for {len(comments)} comments from video {video_id}")
  return comments

def create_comments_dataframe(comments) :
  df=pd.DataFrame(comments)
  if 'published_at' in df.columns:
    df['published_at']=pd.to_datetime(df['published_at'])
  if 'text' in df.columns:
        df['text_length'] = df['text'].str.len()
        df['word_count'] = df['text'].str.split().str.len()
  logger.info(f"Created comments DataFrame with {len(df)} rows and {len(df.columns)} columns")
  return df

def extract_channel_features(channel_data) :
  channels=[]
  for item in channel_data.get('items',[]):
        channel={}
        channel['channel_id'] = item.get('id', '')

        snippet = item.get('snippet', {})
        channel['title'] = snippet.get('title', '')
        channel['description'] = snippet.get('description', '')
        channel['published_at'] = snippet.get('publishedAt', '')
        channel['country'] = snippet.get('country', '')


        content_details = item.get('contentDetails', {})
        related_playlists = content_details.get('relatedPlaylists', {})
        channel['uploads_playlist_id'] = related_playlists.get('uploads', '')


        statistics = item.get('statistics', {})
        channel['view_count'] = int(statistics.get('viewCount', 0))
        channel['subscriber_count'] = int(statistics.get('subscriberCount', 0))
        channel['video_count'] = int(statistics.get('videoCount', 0))

        channels.append(channel)

  logger.info(f"Extracted features for {len(channels)} channels")
  return channels

def create_channel_dataframe(channels) :
  df=pd.DataFrame(channels)
  if 'published_at' in df.columns:
    if 'published_at' in df.columns:
        df['published_at'] = pd.to_datetime(df['published_at'])

    if 'published_at' in df.columns:
        df['channel_age_days'] = (pd.Timestamp.now() - df['published_at']).dt.days

    if all(col in df.columns for col in ['subscriber_count', 'video_count']):
        df['subscribers_per_video'] = df['subscriber_count'] / df['video_count'].replace(0, np.nan)
        df['subscribers_per_video'] = df['subscribers_per_video'].fillna(0)

    logger.info(f"Created channel DataFrame with {len(df)} rows and {len(df.columns)} columns")
    return df

def anonymize_dataframe(df, columns_to_mask) :

    anonymized_df = df.copy()

    for column in columns_to_mask:
        if column in anonymized_df.columns:
            if anonymized_df[column].dtype == 'object':

                anonymized_df[column] = anonymized_df[column].apply(
                    lambda x: f"Masked_{hash(str(x)) % 10000:04d}" if x else x
                )

    logger.info(f"Anonymized {len(columns_to_mask)} columns in DataFrame")
    return anonymized_df

def save_dataframe_to_csv(df, filename) :

    df.to_csv(filename, index=False)
    logger.info(f"Saved DataFrame with {len(df)} rows to {filename}")

