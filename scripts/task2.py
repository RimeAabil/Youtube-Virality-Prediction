# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVxFLP_FATEwHhsSRhvaibKiw2EN8fLM
"""

#%pip install -r requirements.txt

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging

# Import custom modules
from advanced_preprocessing import (
    calculate_virality_score,
    extract_title_features,
    extract_temporal_features,
    extract_channel_features,
    extract_video_metadata_features,
    create_interaction_features,
    handle_missing_values,
    remove_outliers,
    scale_features,
    encode_categorical_features,
    preprocess_pipeline
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set(font_scale=1.2)

def setup_directories(data_dir="youtube_data"):
    """Create necessary directories for the pipeline"""
    processed_dir = f"{data_dir}/processed"
    features_dir = f"{data_dir}/features"
    reports_dir = f"{data_dir}/reports"
    plots_dir = f"{data_dir}/plots"

    # Create directories
    for directory in [data_dir, processed_dir, features_dir, reports_dir, plots_dir]:
        os.makedirs(directory, exist_ok=True)

    logger.info(f"Directories created: {data_dir}, {processed_dir}, {features_dir}, {reports_dir}, {plots_dir}")

    return {
        'data_dir': data_dir,
        'processed_dir': processed_dir,
        'features_dir': features_dir,
        'reports_dir': reports_dir,
        'plots_dir': plots_dir
    }

def inspect_data_types(df):
    """Inspect and print data types for debugging"""
    print("\n" + "="*60)
    print("DATA TYPE INSPECTION")
    print("="*60)
    
    print("\nColumn Data Types:")
    print(df.dtypes)
    
    print("\nSample values for key columns:")
    key_cols = ['view_count', 'like_count', 'comment_count', 'published_at']
    for col in key_cols:
        if col in df.columns:
            print(f"\n{col}:")
            print(f"  Type: {df[col].dtype}")
            print(f"  Sample: {df[col].head(3).tolist()}")
            print(f"  Has NaN: {df[col].isna().any()}")
    
    print("="*60 + "\n")

def load_data(data_dir="youtube_data"):

    logger.info("Loading data from Task 1...")

    # Create the data directory if it doesn't exist
    os.makedirs(data_dir, exist_ok=True)
    logger.info(f"Data directory created/verified: {data_dir}")

    # Try to load from CSV first
    csv_path = f"{data_dir}/processed_videos.csv"
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        logger.info(f"Loaded {len(df)} videos from {csv_path}")
        inspect_data_types(df)
        return df

    # Try alternative CSV path
    csv_path_alt = "processed_videos.csv"
    if os.path.exists(csv_path_alt):
        df = pd.read_csv(csv_path_alt)
        logger.info(f"Loaded {len(df)} videos from {csv_path_alt}")
        inspect_data_types(df)
        return df

    # Otherwise load from JSON
    json_path = f"{data_dir}/all_videos.json"
    if os.path.exists(json_path):
        logger.info(f"Loading data from {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Convert to DataFrame
        df = pd.DataFrame(data)
        logger.info(f"Loaded {len(df)} videos from {json_path}")

        # Inspect data types
        inspect_data_types(df)

        # Save as CSV for future use
        df.to_csv(csv_path, index=False)
        logger.info(f"Saved processed data to {csv_path}")

        return df
    # Try alternative JSON path
    json_path_alt = "all_videos.json"
    if os.path.exists(json_path_alt):
        logger.info(f"Loading data from {json_path_alt}")
        with open(json_path_alt, 'r', encoding='utf-8') as f:
            data = json.load(f)

        df = pd.DataFrame(data)
        logger.info(f"Loaded {len(df)} videos from {json_path_alt}")

        # Inspect data types
        inspect_data_types(df)

        # Save as CSV for future use
        df.to_csv(csv_path, index=False)
        logger.info(f"Saved processed data to {csv_path}")

        return df

    logger.error("No data files found! Please check the following locations:")
    logger.error(f"  - {csv_path}")
    logger.error(f"  - {json_path}")
    logger.error(f"  - {csv_path_alt}")
    logger.error(f"  - {json_path_alt}")
    raise FileNotFoundError("No data files found! Please run Task 1 first or upload data files.")

def run_feature_engineering(df, remove_outliers_flag=True, scale_features_flag=False):
    logger.info("Starting feature engineering pipeline...")

    # Run preprocessing pipeline with correct parameter names
    df_features = preprocess_pipeline(
        video_df=df,
        channel_df=None,  # Add channel data if available
        remove_outliers_flag=remove_outliers_flag,  # Fixed parameter name
        scale_features_flag=scale_features_flag     # Fixed parameter name
    )

    logger.info(f"Feature engineering completed. Shape: {df_features.shape}")

    return df_features

def analyze_features(df, plots_dir, reports_dir):
    """Analyze engineered features"""
    logger.info("Analyzing engineered features...")

    # Feature importance based on correlation with virality
    if 'is_viral' in df.columns:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        correlations = df[numeric_cols].corrwith(df['is_viral']).abs().sort_values(ascending=False)

        # Plot top 20 features
        plt.figure(figsize=(12, 8))
        correlations.head(20).plot(kind='barh')
        plt.title('Top 20 Features Correlated with Virality')
        plt.xlabel('Absolute Correlation')
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/feature_importance.png", dpi=300, bbox_inches='tight')
        plt.show()
        plt.close()

        logger.info("Feature importance plot saved")

        # Save correlation report
        corr_report = correlations.to_dict()
        with open(f"{reports_dir}/feature_correlations.json", 'w') as f:
            json.dump(corr_report, f, indent=2)

        print("\nTop 10 Features Correlated with Virality:")
        print(correlations.head(10))

def visualize_distributions(df, plots_dir):
    """Create visualizations for key features"""
    logger.info("Creating feature distribution plots...")

    # 1. Title Length vs Virality
    if 'title_length' in df.columns and 'is_viral' in df.columns:
        plt.figure(figsize=(10, 6))
        df[df['is_viral'] == 0]['title_length'].hist(alpha=0.6, bins=30, label='Non-Viral', color='blue')
        df[df['is_viral'] == 1]['title_length'].hist(alpha=0.6, bins=30, label='Viral', color='red')
        plt.xlabel('Title Length')
        plt.ylabel('Frequency')
        plt.title('Title Length Distribution: Viral vs Non-Viral Videos')
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/title_length_distribution.png", dpi=300)
        plt.show()
        plt.close()

    # 2. Publication Hour Distribution
    if 'publish_hour' in df.columns and 'is_viral' in df.columns:
        plt.figure(figsize=(12, 6))
        viral_df = df[df['is_viral'] == 1]
        non_viral_df = df[df['is_viral'] == 0]

        plt.hist([non_viral_df['publish_hour'], viral_df['publish_hour']],
                bins=24, alpha=0.6, label=['Non-Viral', 'Viral'])
        plt.xlabel('Hour of Day')
        plt.ylabel('Number of Videos')
        plt.title('Publication Hour Distribution: Viral vs Non-Viral Videos')
        plt.legend()
        plt.xticks(range(0, 24))
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/publication_hour_distribution.png", dpi=300)
        plt.show()
        plt.close()

    # 3. Weekend vs Weekday Performance
    if 'is_weekend' in df.columns and 'virality_score' in df.columns:
        plt.figure(figsize=(10, 6))
        df.boxplot(column='virality_score', by='is_weekend')
        plt.xlabel('Is Weekend (0=Weekday, 1=Weekend)')
        plt.ylabel('Virality Score')
        plt.title('Virality Score: Weekday vs Weekend')
        plt.suptitle('')  # Remove default title
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/weekend_performance.png", dpi=300)
        plt.show()
        plt.close()

    # 4. Peak Hour Performance
    if 'is_peak_hour' in df.columns and 'virality_score' in df.columns:
        plt.figure(figsize=(10, 6))
        df.boxplot(column='virality_score', by='is_peak_hour')
        plt.xlabel('Is Peak Hour (0=Off-Peak, 1=Peak)')
        plt.ylabel('Virality Score')
        plt.title('Virality Score: Peak vs Off-Peak Hours')
        plt.suptitle('')
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/peak_hour_performance.png", dpi=300)
        plt.show()
        plt.close()

    # 5. Emoji Impact
    if 'has_emoji' in df.columns and 'virality_score' in df.columns:
        plt.figure(figsize=(10, 6))
        emoji_means = df.groupby('has_emoji')['virality_score'].mean()
        emoji_means.plot(kind='bar', color=['skyblue', 'coral'])
        plt.xlabel('Has Emoji in Title')
        plt.ylabel('Average Virality Score')
        plt.title('Impact of Emojis on Video Virality')
        plt.xticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/emoji_impact.png", dpi=300)
        plt.show()
        plt.close()

    # 6. Exclamation Mark Impact
    if 'has_exclamation' in df.columns and 'virality_score' in df.columns:
        plt.figure(figsize=(10, 6))
        excl_means = df.groupby('has_exclamation')['virality_score'].mean()
        excl_means.plot(kind='bar', color=['lightgreen', 'orange'])
        plt.xlabel('Has Exclamation Mark in Title')
        plt.ylabel('Average Virality Score')
        plt.title('Impact of Exclamation Marks on Video Virality')
        plt.xticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/exclamation_impact.png", dpi=300)
        plt.show()
        plt.close()

    # 7. Correlation Heatmap
    numeric_features = [
        'title_length', 'has_exclamation', 'has_emoji',
        'is_weekend', 'is_peak_hour', 'view_count',
        'like_count', 'comment_count', 'virality_score'
    ]
    available_features = [f for f in numeric_features if f in df.columns]

    if len(available_features) > 2:
        plt.figure(figsize=(12, 10))
        correlation_matrix = df[available_features].corr()
        sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)
        plt.title('Feature Correlation Heatmap')
        plt.tight_layout()
        plt.savefig(f"{plots_dir}/correlation_heatmap.png", dpi=300)
        plt.show()
        plt.close()

    logger.info("All visualization plots saved")

def generate_summary_report(df, reports_dir):
    """Generate comprehensive summary report"""
    logger.info("Generating summary report...")

    report = {
        'pipeline_execution_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'dataset_info': {
            'total_videos': len(df),
            'total_features': len(df.columns),
            'viral_videos': int(df['is_viral'].sum()) if 'is_viral' in df.columns else None,
            'non_viral_videos': int((1 - df['is_viral']).sum()) if 'is_viral' in df.columns else None,
        },
        'feature_categories': {
            'title_features': [
                'title_length', 'title_word_count', 'has_exclamation',
                'has_question', 'emoji_count', 'has_emoji', 'caps_word_count',
                'has_numbers', 'special_char_count', 'clickbait_score'
            ],
            'temporal_features': [
                'publish_hour', 'publish_month', 'is_weekend', 'is_peak_hour',
                'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos'
            ],
            'engagement_features': [
                'view_count', 'like_count', 'comment_count', 'engagement_rate',
                'view_to_like_ratio', 'view_to_comment_ratio'
            ],
            'target_variables': [
                'virality_score', 'is_viral'
            ]
        },
        'data_quality': {
            'missing_values': int(df.isnull().sum().sum()),
            'duplicate_rows': int(df.duplicated().sum()),
        },
        'key_insights': {}
    }

    # Add key insights
    if 'is_viral' in df.columns:
        # Weekend performance
        if 'is_weekend' in df.columns:
            weekend_viral_rate = df[df['is_weekend'] == 1]['is_viral'].mean()
            weekday_viral_rate = df[df['is_weekend'] == 0]['is_viral'].mean()
            report['key_insights']['weekend_advantage'] = {
                'weekend_viral_rate': f"{weekend_viral_rate*100:.1f}%",
                'weekday_viral_rate': f"{weekday_viral_rate*100:.1f}%",
            }

        # Peak hour performance
        if 'is_peak_hour' in df.columns:
            peak_viral_rate = df[df['is_peak_hour'] == 1]['is_viral'].mean()
            offpeak_viral_rate = df[df['is_peak_hour'] == 0]['is_viral'].mean()
            report['key_insights']['peak_hour_advantage'] = {
                'peak_hour_viral_rate': f"{peak_viral_rate*100:.1f}%",
                'off_peak_viral_rate': f"{offpeak_viral_rate*100:.1f}%",
            }

        # Emoji impact
        if 'has_emoji' in df.columns:
            emoji_viral_rate = df[df['has_emoji'] == 1]['is_viral'].mean()
            no_emoji_viral_rate = df[df['has_emoji'] == 0]['is_viral'].mean()
            report['key_insights']['emoji_impact'] = {
                'with_emoji_viral_rate': f"{emoji_viral_rate*100:.1f}%",
                'without_emoji_viral_rate': f"{no_emoji_viral_rate*100:.1f}%",
            }

    # Save report
    report_path = f"{reports_dir}/preprocessing_summary_report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)

    logger.info(f"Summary report saved to {report_path}")

    # Also create markdown version
    create_markdown_report(report, reports_dir)

    # Display summary
    print("\n" + "="*60)
    print("PREPROCESSING SUMMARY")
    print("="*60)
    print(f"Total Videos: {report['dataset_info']['total_videos']}")
    print(f"Total Features: {report['dataset_info']['total_features']}")
    print(f"Viral Videos: {report['dataset_info']['viral_videos']}")
    print(f"Non-Viral Videos: {report['dataset_info']['non_viral_videos']}")
    print(f"Missing Values: {report['data_quality']['missing_values']}")
    print(f"Duplicate Rows: {report['data_quality']['duplicate_rows']}")
    print("="*60)

    if report['key_insights']:
        print("\nKEY INSIGHTS:")
        for key, value in report['key_insights'].items():
            print(f"\n{key.replace('_', ' ').title()}:")
            for k, v in value.items():
                print(f"  - {k.replace('_', ' ').title()}: {v}")

    return report

def create_markdown_report(report, reports_dir):
    """Create markdown version of the report"""
    md_content = f"""# YouTube Video Virality Prediction - Preprocessing Report

**Generated:** {report['pipeline_execution_date']}

## Dataset Overview

- **Total Videos:** {report['dataset_info']['total_videos']}
- **Total Features:** {report['dataset_info']['total_features']}
- **Viral Videos:** {report['dataset_info'].get('viral_videos', 'N/A')}
- **Non-Viral Videos:** {report['dataset_info'].get('non_viral_videos', 'N/A')}

## Feature Categories

### Title Features
{chr(10).join([f"- {f}" for f in report['feature_categories']['title_features']])}

### Temporal Features
{chr(10).join([f"- {f}" for f in report['feature_categories']['temporal_features']])}

### Engagement Features
{chr(10).join([f"- {f}" for f in report['feature_categories']['engagement_features']])}

### Target Variables
{chr(10).join([f"- {f}" for f in report['feature_categories']['target_variables']])}

## Data Quality

- **Missing Values:** {report['data_quality']['missing_values']}
- **Duplicate Rows:** {report['data_quality']['duplicate_rows']}

## Key Insights

"""

    # Add insights
    for key, value in report.get('key_insights', {}).items():
        md_content += f"### {key.replace('_', ' ').title()}\n"
        for k, v in value.items():
            md_content += f"- **{k.replace('_', ' ').title()}:** {v}\n"
        md_content += "\n"

    md_content += """
## Next Steps

1. **Model Training:** Use the engineered features to train classification models
2. **Feature Selection:** Apply feature selection techniques to identify most important predictors
3. **Model Evaluation:** Test different algorithms (Random Forest, XGBoost, Neural Networks)
4. **Deployment:** Create API endpoint for real-time virality prediction
5. **Monitoring:** Set up Airflow pipeline for continuous data collection and model retraining

## Visualizations

All visualization plots have been saved to the `plots/` directory:
- Feature importance analysis
- Title length distribution
- Publication hour patterns
- Weekend vs weekday performance
- Peak hour analysis
- Emoji and exclamation mark impact
- Feature correlation heatmap
"""

    md_path = f"{reports_dir}/preprocessing_summary_report.md"
    with open(md_path, 'w') as f:
        f.write(md_content)

    logger.info(f"Markdown report saved to {md_path}")

def save_processed_data(df, features_dir):
    """Save processed and engineered dataset"""
    # Save complete dataset
    output_path = f"{features_dir}/engineered_features.csv"
    df.to_csv(output_path, index=False)
    logger.info(f"Engineered features saved to {output_path}")

    # Save feature-target split
    if 'is_viral' in df.columns:
        # Separate features and target
        target_cols = ['is_viral', 'virality_score', 'video_id', 'title', 'published_at']
        feature_cols = [col for col in df.columns if col not in target_cols]

        X = df[feature_cols]
        y = df[['is_viral', 'virality_score']]

        X.to_csv(f"{features_dir}/X_features.csv", index=False)
        y.to_csv(f"{features_dir}/y_target.csv", index=False)

        logger.info("Feature-target split saved")

        print(f"\nSaved files:")
        print(f"  - {output_path}")
        print(f"  - {features_dir}/X_features.csv")
        print(f"  - {features_dir}/y_target.csv")

def run_preprocessing_pipeline(data_dir="youtube_data", remove_outliers_flag=True, scale_features_flag=False):
    """
    Execute complete preprocessing pipeline

    Args:
        data_dir: Directory containing the data
        remove_outliers_flag: Whether to remove outliers
        scale_features_flag: Whether to scale features

    Returns:
        Tuple of (processed_dataframe, report_dict, directories_dict)
    """
    logger.info("="*60)
    logger.info("STARTING PREPROCESSING PIPELINE")
    logger.info("="*60)

    try:
        # Setup directories
        dirs = setup_directories(data_dir)

        # Load data
        df = load_data(data_dir)
        logger.info(f"Initial data shape: {df.shape}")
        print(f"\nInitial data shape: {df.shape}")

        # Run feature engineering
        df_features = run_feature_engineering(df, remove_outliers_flag, scale_features_flag)
        logger.info(f"Engineered data shape: {df_features.shape}")
        print(f"Engineered data shape: {df_features.shape}")

        # Analyze features
        analyze_features(df_features, dirs['plots_dir'], dirs['reports_dir'])

        # Create visualizations
        visualize_distributions(df_features, dirs['plots_dir'])

        # Generate report
        report = generate_summary_report(df_features, dirs['reports_dir'])

        # Save processed data
        save_processed_data(df_features, dirs['features_dir'])

        logger.info("="*60)
        logger.info("PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY!")
        logger.info("="*60)
        logger.info(f"Final dataset: {df_features.shape[0]} videos, {df_features.shape[1]} features")
        logger.info(f"Reports saved in: {dirs['reports_dir']}")
        logger.info(f"Plots saved in: {dirs['plots_dir']}")
        logger.info(f"Features saved in: {dirs['features_dir']}")

        print("\n" + "="*60)
        print("PIPELINE COMPLETED SUCCESSFULLY!")
        print("="*60)
        print(f"Final dataset: {df_features.shape[0]} videos, {df_features.shape[1]} features")
        print(f"Reports saved in: {dirs['reports_dir']}")
        print(f"Plots saved in: {dirs['plots_dir']}")
        print(f"Features saved in: {dirs['features_dir']}")
        print("="*60)

        return df_features, report, dirs

    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        raise